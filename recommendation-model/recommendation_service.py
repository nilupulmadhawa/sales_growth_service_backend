# -*- coding: utf-8 -*-
"""recommendation_service.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19a2NwolqxmACld61seiINbHMm3wJ2g-Q
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install lightfm

import pandas as pd
import numpy as np
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.evaluation import precision_at_k, auc_score, reciprocal_rank
from lightfm.cross_validation import random_train_test_split
import matplotlib.pyplot as plt
import joblib

# Load your data
users_df = pd.read_csv('/content/drive/MyDrive/recommendation_service_dataset/users.csv')
products_df = pd.read_csv('/content/drive/MyDrive/recommendation_service_dataset/products.csv')
events_df = pd.read_csv('/content/drive/MyDrive/recommendation_service_dataset/events.csv')

# Preprocessing
event_type_weights = {
    'purchase': 3.0,   # High weight as it directly indicates a preference.
    'cart': 2.5,       # Adding to cart is a strong buying signal.
    'product': 2.0,    # Viewing a product shows interest.
    'department': 1.0, # Browsing a department shows mild interest.
    'cancel': 0.5,     # Cancelling might indicate disinterest.
    'home': 0.5        # Visiting the home page is generic, low informational value.
}

events_df['event_weight'] = events_df['event_type'].map(event_type_weights)

events_df['product_id'] = events_df.apply(lambda row: int(row['uri'].split('/')[-1]) if '/product/' in row['uri'] else np.nan, axis=1)

events_df = events_df.dropna(subset=['product_id', 'user_id'])

events_df.loc[:, 'user_id'] = events_df['user_id'].astype(int)
events_df.loc[:, 'product_id'] = events_df['product_id'].astype(int)

filtered_events = events_df[events_df['user_id'].isin(users_df['id']) & events_df['product_id'].isin(products_df['id'])]

# Add age group feature to users
users_df['age_group'] = pd.cut(users_df['age'], bins=[0, 18, 25, 35, 45, 55, 65, 100], labels=['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '65+'])


print(filtered_events.head())
print(users_df.head())

from lightfm.data import Dataset

# Initialize the dataset
dataset = Dataset()

# Fit the dataset with the user IDs, item IDs, and declare the features to use
dataset.fit(
    users=(x for x in users_df['id']),
    items=(x for x in products_df['id']),
    user_features=(f"{row['gender']}_{row['age_group']}_{row['state']}" for index, row in users_df.iterrows()),
    item_features=(f"{row['category']}_{row['brand']}_{row['department']}" for index, row in products_df.iterrows())
)

(interactions_matrix, weights_matrix) = dataset.build_interactions(
    (row['user_id'], row['product_id'], row['event_weight'])
    for index, row in events_df.iterrows()
)

user_features = dataset.build_user_features(
    (row['id'], [f"{row['gender']}_{row['age_group']}_{row['state']}"])
    for index, row in users_df[users_df['id'].isin(filtered_events['user_id'])].iterrows()
)

item_features = dataset.build_item_features(
    (row['id'], [f"{row['category']}_{row['brand']}_{row['department']}"])
    for index, row in products_df.iterrows()
)

import matplotlib.pyplot as plt

# Assuming 'filtered_events' is your final events DataFrame after preprocessing
# Interactions per User
user_interactions = filtered_events.groupby('user_id').size()
item_interactions = filtered_events.groupby('product_id').size()

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
user_interactions.hist(bins=30, edgecolor='black')
plt.title('Interactions per User')
plt.xlabel('Number of Interactions')
plt.ylabel('Number of Users')

plt.subplot(1, 2, 2)
item_interactions.hist(bins=30, edgecolor='black')
plt.title('Interactions per Item')
plt.xlabel('Number of Interactions')
plt.ylabel('Number of Items')

plt.tight_layout()
plt.show()

# Split the data into training and testing sets
train, test = random_train_test_split(interactions_matrix, test_percentage=0.2, random_state=np.random.RandomState(779))

# Initialize the LightFM model with WARP loss function
# Adjust these parameters to optimize the gap stability
learning_rate = 0.03  # Lower learning rate
no_components = 30    # Fewer factors to reduce complexity
user_alpha = 1e-4     # Higher regularization
item_alpha = 1e-4     # Higher regularization


# Initialize the model with adjusted parameters for better performance and regularization
model = LightFM(loss='warp', no_components=no_components,
                user_alpha=user_alpha, item_alpha=item_alpha,
                learning_schedule='adagrad', learning_rate=learning_rate, random_state=1616)

# Initialize lists to store the metrics
train_auc_list, test_auc_list, gap_list = [], [], []
train_rr_list, test_rr_list = [], []

num_epochs = 50
evaluation_interval = 5
best_gap = 0.05
patience = 3
patience_counter = 0
num_thread = 4

for epoch in range(num_epochs):
    model.fit_partial(train, user_features=user_features, item_features=item_features, epochs=1,
                      num_threads=num_thread)

    if (epoch + 1) % evaluation_interval == 0:
        train_rr = reciprocal_rank(model, train, user_features=user_features, item_features=item_features).mean()
        test_rr = reciprocal_rank(model, test, user_features=user_features, item_features=item_features).mean()
        train_auc = auc_score(model, train, user_features=user_features, item_features=item_features).mean()
        test_auc = auc_score(model, test, user_features=user_features, item_features=item_features).mean()

        current_gap = train_auc - test_auc
        train_rr_list.append(train_rr)
        test_rr_list.append(test_rr)
        train_auc_list.append(train_auc)
        test_auc_list.append(test_auc)
        gap_list.append(current_gap)

        print(f'Epoch: {epoch+1} - Train RR: {train_rr:.4f}, Test RR: {test_rr:.4f}')
        print(f'Epoch: {epoch+1} - Train AUC: {train_auc:.4f}, Test AUC: {test_auc:.4f}, Gap: {current_gap:.4f}')

        # Monitor the gap and apply early stopping if it exceeds the threshold
        if current_gap < best_gap:
            patience_counter = 0
            joblib.dump(model, '/content/best_recommendation_hybrid_model.pkl')  # Save the best model
        else:
            patience_counter += 1
            if patience_counter >= patience or current_gap > 0.05:
                print(f"Early stopping triggered due to gap exceeding {best_gap} or no improvement")
                joblib.dump(model, '/content/recommendation_hybrid_model.pkl')
                break

# Final evaluation at the end of training
train_precision = precision_at_k(model, train, k=10, user_features=user_features, item_features=item_features).mean()
test_precision = precision_at_k(model, test, k=10, user_features=user_features, item_features=item_features).mean()
train_auc = auc_score(model, train, user_features=user_features, item_features=item_features).mean()
test_auc = auc_score(model, test, user_features=user_features, item_features=item_features).mean()

print('Final evaluation results:')
print(f'Precision: Train {train_precision:.4f}, Test {test_precision:.4f}')
print(f'AUC: Train {train_auc:.4f}, Test {test_auc:.4f}')

# Sample recommendation function
def sample_recommendation(model, user_id, user_features, item_features, dataset, num_items=10):
    n_users, n_items = dataset.interactions_shape()
    user_x = dataset.mapping()[0][user_id]
    scores = model.predict(user_x, np.arange(n_items), user_features=user_features, item_features=item_features)
    top_items = np.argsort(-scores)[:num_items]

    return [dataset.mapping()[2][i] for i in top_items]

# Test the recommendation function for a specific user 17033
user_id = 1
recommended_products = sample_recommendation(model, user_id, user_features, item_features, dataset)
print(f'Recommended product IDs for user {user_id}: {recommended_products}')

joblib.dump(model, 'recommend_model.pkl')

plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.plot(train_rr_list, label='Train RR')
plt.plot(test_rr_list, label='Test RR')
plt.title('Reciprocal Rank over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Reciprocal Rank')
plt.legend()

plt.subplot(122)
plt.plot(train_auc_list, label='Train AUC')
plt.plot(test_auc_list, label='Test AUC')
plt.title('AUC over Epochs')
plt.xlabel('Epochs')
plt.ylabel('AUC')
plt.legend()

plt.tight_layout()
plt.show()

model = joblib.load('recommendation_hybrid_model.pkl')
test_prediction = model.predict(0, np.arange(10), user_features=user_features, item_features=item_features)
print(test_prediction)

# Check if model can predict without error
try:
    test_prediction = model.predict(0, np.arange(10), user_features=user_features, item_features=item_features)
    print("Prediction successful:", test_prediction)
except Exception as e:
    print("Error during prediction:", str(e))

def similar_items(item_id, model, item_features, N=10):
    # Get item representations (biases and features)
    item_bias, item_representations = model.get_item_representations(features=item_features)
    # Compute cosine similarity
    target_item_representation = item_representations[item_id]
    scores = item_representations.dot(target_item_representation)
    top_indices = np.argsort(-scores)[:N]
    return top_indices

# Example of finding similar items
item_id = 13842
similar_ids = similar_items(item_id, model, item_features)
print("Similar Items to Item ID 10:", similar_ids)

def similar_users(user_id, model, user_features, N=10):
    user_bias, user_representations = model.get_user_representations(features=user_features)
    target_user_representation = user_representations[user_id]
    scores = user_representations.dot(target_user_representation)
    top_indices = np.argsort(-scores)[:N]
    return top_indices

# Example of finding similar items
user_id = 39
similar_ids = similar_users(user_id, model, user_features)
print("Similar users:", similar_ids)

def generate_recommendations(model, user_id, user_features, item_features, dataset, products_df, num_items=10):
    user_x = dataset.mapping()[0][user_id]
    scores = model.predict(user_x, np.arange(dataset.interactions_shape()[1]), user_features=user_features, item_features=item_features)
    top_items_indices = np.argsort(-scores)[:num_items]
    top_items_ids = [dataset.mapping()[2][i] for i in top_items_indices]
    # Get item names or other details from products_df
    top_items_details = products_df[products_df['id'].isin(top_items_ids)][['id', 'name']]
    return top_items_details

# Example of finding similar items
user_id = 39 # You need to replace this with a valid item ID from your dataset
recommendations = generate_recommendations(model, user_id, user_features, item_features, dataset,products_df, num_items=10)
print(recommendations)

"""Solution for Cold Start Problem"""

from scipy.sparse import csr_matrix

# Define user features as a dictionary or similar structure
features_dict = {
    'gender_male': 1,
    'age_25': 1,
    'location_Daegu': 1,
    'brand_Perry_Ellis': 1
}

# Assuming the total number of features is known and indexed
num_features = 100  # Adjust this to the actual number of different feature tokens you have
feature_indices = {
    'gender_male': 0,
    'age_25': 1,
    'location_Daegu': 2,
    'brand_Perry_Ellis': 3
}

# Create feature vector
feature_vector = [0] * num_features
for feature, index in feature_indices.items():
    if feature in features_dict:
        feature_vector[index] = features_dict[feature]

# Convert to CSR matrix
user_features_csr = csr_matrix(feature_vector)

# Load the model
model = joblib.load('recommendation_hybrid_model.pkl')

# Predicting with a manually created CSR matrix
num_items = dataset.interactions_shape()[1]  # Number of items
new_user_predictions = model.predict(0, np.arange(num_items), user_features=user_features_csr, item_features=item_features)

# Output the predictions
print("Predictions:", new_user_predictions)

import numpy as np
import joblib

# Load the model
model = joblib.load('recommendation_hybrid_model.pkl')

# Predicting with a manually created CSR matrix
num_items = dataset.interactions_shape()[1]  # Number of items
new_user_predictions = model.predict(0, np.arange(num_items), user_features=user_features_csr, item_features=item_features)

# Extract the indices of the top items based on scores
top_items_indices = np.argsort(-new_user_predictions)[:10]

# Get the IDs of the top items from the indices
top_item_ids = products_df.iloc[top_items_indices]['id']

# Fetch details for these top items from your products DataFrame
top_items_details = products_df[products_df['id'].isin(top_item_ids)][['id', 'name']]

# Output the top item details
print("Top Recommended Items:")
print(top_items_details)

from scipy.sparse import csr_matrix

# Updated user features for a female user
features_dict = {
    'gender_female': 1,  # Assuming the model was trained with this feature
    'age_25': 1,
    'location_Daegu': 1,
    'brand_Perry_Ellis': 1
}

# Assuming the total number of features is known and indexed, update accordingly
num_features = 100  # This should match your model's feature setup
feature_indices = {
    'gender_female': 1,  # Update the index as per your model's feature indexing
    'age_25': 1,
    'location_Daegu': 2,
    'brand_Perry_Ellis': 3
}

# Create the feature vector based on the defined features
feature_vector = [0] * num_features
for feature, index in feature_indices.items():
    if feature in features_dict:
        feature_vector[index] = features_dict[feature]

# Convert to CSR matrix
user_features_csr = csr_matrix(feature_vector)

# Load the model
model = joblib.load('recommendation_hybrid_model.pkl')

# Predicting with a manually created CSR matrix
num_items = dataset.interactions_shape()[1]  # Number of items
new_user_predictions = model.predict(0, np.arange(num_items), user_features=user_features_csr, item_features=item_features)

# Output the predictions
print("Predictions:", new_user_predictions)

# Predicting with a manually created CSR matrix
num_items = dataset.interactions_shape()[1]  # Number of items
new_user_predictions = model.predict(0, np.arange(num_items), user_features=user_features_csr, item_features=item_features)

# Extract the indices of the top items based on scores
top_items_indices = np.argsort(-new_user_predictions)[:20]

# Get the IDs of the top items from the indices
top_item_ids = products_df.iloc[top_items_indices]['id']

# Fetch details for these top items from your products DataFrame
top_items_details = products_df[products_df['id'].isin(top_item_ids)][['id', 'name']]

# Output the top item details
print("Top Recommended Items:")
print(top_items_details)